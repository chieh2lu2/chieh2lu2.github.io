---
title: 从零开始的爬虫指北
comment: true
aging: true
mathjax: true
code_block_shrink: false
date: 2024-12-01 21:29:49
tags:
 - Python
 - 爬虫
 - 计算机社
 - 永中
categories: 指北

---

# 0. 为啥要学爬虫？
有无在网站上手动一张一张下载图片时感到麻烦？  
可否好奇过网上所谓的 [绘板活动](https://www.bilibili.com/video/BV1Y94y1e7g3/) 大家是如何参与其中的？  
每每要获得一个网页中若干子网页的特定内容，是否感到烦恼？  
希望保存一些视频、数据之类，但却无从下手？  
爬虫会帮你解决上述的问题！

虽然计算机社活动的时间很短，所以讲不了太深的东西（~~主要我自己没有实力~~），只能介绍比较简单和常见的爬虫写法。  
但我仍然希望大家能捞到点东西，有兴趣的话可以自己深入学学。~~其实爬虫是可以赚钱的，大学里可以搞搞兼职~~

---

# 1. 什么是爬虫？
> 网络爬虫（又称为网页蜘蛛，网络机器人，在 FOAF 社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。——百度百科

显然这不是我们能看得懂的。简单点说，爬虫就是你写的一个程序，它能帮你在网上爬取你想要的东西，或者是帮你做点重复性的网页工作。百度要实现搜索引擎的功能，就要先用爬虫获取数据。

### ……那我是不是啥都能爬，包括重要机密那种？
首先，任何网站都有一个 `robots.txt`，里面描述了网站管理者不希望爬虫做的事，如不要访问某些文件夹、限制访问网站的频率等等。想要查看一个网站的 `robots.txt` 的话，只需网站的根域名 + `/robots.txt` 即可。例如，迷你世界官网的 `robots.txt` 就在 [`https://www.mini1.cn/robots.txt`](https://www.mini1.cn/robots.txt) 查看。~~（不过貌似是空的）~~
其次，网页也有许多的反爬机制以限制爬虫。

我们写的爬虫要遵守 `robots.txt`，以及限制我们访问的频率。



# 2. 微编写两个小爬虫

在这之前我先介绍一下 Python 的文件读写操作。

Python 使用 `open(dir, mode)` 读取文件。其中，`dir` 为一个文件的绝对路径或相对路径的字符串，`mode` 决定了打开文件的模式。如果不写这个参数，那默认为只读（`r`）。下面列出一些常见的 `mode` 参数：

|`mode` 参数|内容|
|-|-|
|`r`|以**只读**方式打开文件。文件的指针将会放在文件的开头。这是默认模式。**文件必须存在，否则会出错。**|
|`r+`|打开一个文件用于**读写**。文件指针将会放在文件的开头。**文件必须存在，否则会出错。**||
|`rb`|以二进制格式打开一个文件用于**只读**。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。**文件必须存在，否则会出错。**||
|`rb+`|以二进制格式打开一个文件用于**读写**。文件指针将会放在文件的开头。一般用于非文本文件如图片等。**文件必须存在，否则会出错。**||
|`w`|打开一个文件**只用于写入**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。|
|`w+`|打开一个文件用于**读写**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。|
|`wb`|以二进制格式打开一个文件**只用于写入**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。|
|`wb+`|以二进制格式打开一个文件用于**读写**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。|
|`a`|打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。|
|`a+`|打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。|
|`ab`|以二进制格式打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。|
|`ab+`|以二进制格式打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。|

简单来说，`r` 读文件，`w` 写文件，`b` 用二进制格式打开，可以打开非文本文件。相比于 `w`，`a` 为追加，在被多次调用时不会覆盖文件原来的内容。`+` 可以「拓展功能」，由原来单一的读、写变成同时可读写模式。**但 `r+` 得保证文件存在。**

`open()` 函数会返回一个 `file` 对象，而这种对象有个 `read()` 方法。调用后，会读取文件里的内容（字节），并以字符串的形式返回。比方说，我们让 `f = open("mnsj.txt", "r")`，如果执行 `print(f.read())`，那就会输出 `mnsj.txt` 中的内容了。

读完文件后需要关闭文件，代码为 `f.close()`。非常多的情况下我们会忘记这一操作，所以另一种方法就是使用 `with`。对比一下两种写法：

```python
# 第一种
f = open("mnsj.txt", "r")
print(f.read())
f.close()

# 第二种
with open("mnsj.txt", "r") as f:
    print(f.read())
```
推荐使用第二种写法。下面列出了 `file` 对象的常用方法：


|`file` 对象的方法|内容|
|-|-|
|`read(size)`|读取 $\text{size}$ 个字符。若 $\text{size}$ 为空或 $\text{size < 0}$ 则读取整个文件。|
|`close()`|关闭文件。|
|`write(content)`|将 `content` 写入到文件中。|

下面我们开始写两个小爬虫。

---

我们使用 Python 来写我们的爬虫代码。我们爬取 [迷你世界官网](https://www.mini1.cn/) 的源代码：

```python
import requests # 引入 requsets 库
response = requests.get('https://www.mini1.cn/') # get 请求
response.encoding = 'utf-8' # 用 utf-8 解码
print(response.text) # 输出
```

成功输出了迷你世界官网的 HTML 代码！有同学会发现，换成某些网站可能会报错。出现这种情况的原因、解决方法与 Request 库的进一步了解我们之后再聊。

接下来是下载图片的爬虫。在网上冲浪时，经常能碰见能够生成随机壁纸的网站以及 API。我们来写一个爬虫把图片下载下来。

```python
import requests
response = requests.get('https://www.dmoe.cc/random.php') # 调用某随机图片 API 下载图片
with open("plmm.jpg", "wb+") as f: # 使用 wb+ 来读写图片
    f.write(response.content) # 写入到 plmm.jpg 中
```

发现同文件夹下出现了 `plmm.jpg` 文件。真的有漂亮美眉！

至此，我们对爬虫已经有了初步的了解。下次活动我们会介绍 HTTP 协议和爬虫发送请求的具体过程，虽然非常枯燥，但是非常重要，请一定要认真听。

# 3. Web 请求过程分析与 HTTPS 协议

在学习 Request 库之前，我们先来了解 Web 请求过程以及 HTTPS 协议。

## 3.1 Web 请求过程

渲染一个网页有**服务器渲染**和**客户端渲染**两种方式。具体过程不做过多介绍。
简单来说，服务器渲染，就是服务端把 HTML 和数据整合在一起发送给客户端；客户端渲染，就是服务端先把 HTML 的「框架」发送给客户端，在客户端再次请求获取数据后，动态地生成网页内容。其中，页面的渲染过程主要由客户端的浏览器完成，因而得名。

什么意思呢？以在百度搜索内容的过程举例。比方说我搜「迷你世界」：

![figure_3.1.1](https://www.imagehub.cc/image/3.1.1.CKNEss)



我们 右键 → 查看页面源代码 来查看页面的 HTML 源代码。在其中搜索「迷你世界」，出现了大量匹配。可见，服务器将我们请求关于「迷你世界」的相关数据写进了 HTML 中，再把这个 HTML 返回给了我们。这属于服务器渲染。

![figure_3.1.2](https://www.imagehub.cc/image/3.1.2.CKN4wZ)

客户端渲染的一个例子是 B 站首页。B 站首页有许多系统推荐的视频，算是客户端索取的数据。然而，我们查看 B 站首页源代码，会发现他空空如也：

![figure_3.1.3](https://www.imagehub.cc/image/3.1.3.CKNGpI)

![figure_3.1.4](https://www.imagehub.cc/image/3.1.4.CKNR1h)

这种就属于客户端渲染。

对于第一种情况我们可以直接爬取网页源代码后再处理源代码；但如果我们面对的是第二种情况呢？这就需要我们使用浏览器的**抓包工具**。按下 F12，选择网络（Network），就可以看到每个网络操作的相关信息，包括详细的耗时数据、HTTP 请求与响应标头和 Cookie。在名称（Name）一栏的第一个就是客户端先收到的 HTML 框架；后面就是客户端陆陆续续渲染的数据了。选择标头（Headers），第一行就是这个数据所在的 URL。

![figure_3.1.5](https://www.imagehub.cc/image/3.1.5.CKNc4G)

关于 Web 请求过程我们就先讲这么多，主要是了解了解抓包的过程。

## 3.2 HTTP 协议

HTTP 协议，Hypertext Transfer Protocol（超文本传输协议）的简写。浏览器和服务器间的数据交互遵守的就是HTTP 协议。

> 通常，由 HTTP 客户端发起一个请求，建立一个到服务器指定端口（默认是 80 端口）的 TCP 连接。HTTP 服务器则在那个端口监听客户端的请求。一旦收到请求，服务器会向客户端返回一个状态，比如 `HTTP/1.1 200 OK`，以及返回的内容，如请求的文件、错误消息、或者其它信息。——维基百科

HTTP 协议的一条消息有请求和相应两种类型。

### 3.2.1 HTTP 请求

请求消息有三块内容：请求行、请求头、请求体。

请求行包括请求方法、资源路径与协议版本。其中，请求方法我们需要了解两种：**GET 方法** 和 **POST 方法**。GET 用于获取数据，如进入一个网页时，获得网页内容；POST 用于创建数据，如用户提交自己的用户名等等。

请求头包含一些给服务器的信息，如 HOST、User-Agent、cookie 等等。User-Agent 表明了客户端的相关信息。

请求体一般放一些请求参数。GET 的请求体一般是空的。

### 3.2.2 HTTP 响应

响应消息有也三块内容：状态行、响应头、响应体。

状态行有协议版本和状态码。`404`（Not Found）就是一种状态码，表明「	服务器无法根据客户端的请求找到资源」。状态码对应的意思（状态信息）可以参考 [这里](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status)。

响应头包含一些给客户端的信息。如日期、编码类型等。

响应体包含的是客户端真正要的内容，如 HTML 等，也就是我们要爬的东西。

### 3.2.3 使用浏览器抓包工具查看上述内容

还是按下 F12，选择网络（Network），选择一个数据，就可以看到请求方法、状态代码等诸多内容。


*******

了解了这么多内容，下次我们是时候该学学 `requests` 这个库咋用了。