---
title: 从零开始的爬虫指北
comment: true
aging: true
mathjax: true
code_block_shrink: false
date: 2024-12-01 21:29:49
tags:
 - Python
 - 爬虫
 - 计算机社
 - 永中
categories: 指北

---

# 0. 为啥要学爬虫？
有无在网站上手动一张一张下载图片时感到麻烦？  
可否好奇过网上所谓的 [绘板活动](https://www.bilibili.com/video/BV1Y94y1e7g3/) 大家是如何参与其中的？  
每每要获得一个网页中若干子网页的特定内容，是否感到烦恼？  
希望保存一些视频、数据之类，但却无从下手？  
爬虫会帮你解决上述的问题！

虽然计算机社活动的时间很短，所以讲不了太深的东西（~~主要我自己没有实力~~），只能介绍比较简单和常见的爬虫写法。  
但我仍然希望大家能捞到点东西，有兴趣的话可以自己深入学学。~~其实爬虫是可以赚钱的，大学里可以搞搞兼职~~

---

# 1. 什么是爬虫？
> 网络爬虫（又称为网页蜘蛛，网络机器人，在 FOAF 社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。——百度百科

显然这不是我们能看得懂的。简单点说，爬虫就是你写的一个程序，它能帮你在网上爬取你想要的东西，或者是帮你做点重复性的网页工作。百度要实现搜索引擎的功能，就要先用爬虫获取数据。

### ……那我是不是啥都能爬，包括重要机密那种？
首先，任何网站都有一个 `robots.txt`，里面描述了网站管理者不希望爬虫做的事，如不要访问某些文件夹、限制访问网站的频率等等。想要查看一个网站的 `robots.txt` 的话，只需网站的根域名 + `/robots.txt` 即可。例如，迷你世界官网的 `robots.txt` 就在 [`https://www.mini1.cn/robots.txt`](https://www.mini1.cn/robots.txt) 查看。~~（不过貌似是空的）~~
其次，网页也有许多的反爬机制以限制爬虫。

我们写的爬虫要遵守 `robots.txt`，以及限制我们访问的频率。

---

# 2. 微编写两个小爬虫

在这之前我先介绍一下 Python 的文件读写操作。

Python 使用 `open(dir, mode)` 读取文件。其中，`dir` 为一个文件的绝对路径或相对路径的字符串，`mode` 决定了打开文件的模式。如果不写这个参数，那默认为只读（`r`）。下面列出一些常见的 `mode` 参数：

|`mode` 参数|内容|
|-|-|
|`r`|以**只读**方式打开文件。文件的指针将会放在文件的开头。这是默认模式。**文件必须存在，否则会出错。**|
|`r+`|打开一个文件用于**读写**。文件指针将会放在文件的开头。**文件必须存在，否则会出错。**||
|`rb`|以二进制格式打开一个文件用于**只读**。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。**文件必须存在，否则会出错。**||
|`rb+`|以二进制格式打开一个文件用于**读写**。文件指针将会放在文件的开头。一般用于非文本文件如图片等。**文件必须存在，否则会出错。**||
|`w`|打开一个文件**只用于写入**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。|
|`w+`|打开一个文件用于**读写**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。|
|`wb`|以二进制格式打开一个文件**只用于写入**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。|
|`wb+`|以二进制格式打开一个文件用于**读写**。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。|
|`a`|打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。|
|`a+`|打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。|
|`ab`|以二进制格式打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。|
|`ab+`|以二进制格式打开一个文件用于**追加**。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。|

简单来说，`r` 读文件，`w` 写文件，`b` 用二进制格式打开，可以打开非文本文件。相比于 `w`，`a` 为追加，在被多次调用时不会覆盖文件原来的内容。`+` 可以「拓展功能」，由原来单一的读、写变成同时可读写模式。**但 `r+` 得保证文件存在。**

`open()` 函数会返回一个 `file` 对象，而这种对象有个 `read()` 方法。调用后，会读取文件里的内容（字节），并以字符串的形式返回。比方说，我们让 `f = open("mnsj.txt", "r")`，如果执行 `print(f.read())`，那就会输出 `mnsj.txt` 中的内容了。

读完文件后需要关闭文件，代码为 `f.close()`。非常多的情况下我们会忘记这一操作，所以另一种方法就是使用 `with`。对比一下两种写法：

```python
# 第一种
f = open("mnsj.txt", "r")
print(f.read())
f.close()

# 第二种
with open("mnsj.txt", "r") as f:
    print(f.read())
```
推荐使用第二种写法。下面列出了 `file` 对象的常用方法：


|`file` 对象的方法|内容|
|-|-|
|`read(size)`|读取 $\text{size}$ 个字符。若 $\text{size}$ 为空或 $\text{size < 0}$ 则读取整个文件。|
|`close()`|关闭文件。|
|`write(content)`|将 `content` 写入到文件中。|

下面我们开始写两个小爬虫。

---

我们使用 Python 来写我们的爬虫代码。我们爬取 [迷你世界官网](https://www.mini1.cn/) 的源代码：

```python
import requests # 引入 requsets 库
response = requests.get('https://www.mini1.cn/') # get 请求
response.encoding = 'utf-8' # 用 utf-8 解码
print(response.text) # 输出
```

成功输出了迷你世界官网的 HTML 代码！有同学会发现，换成某些网站可能会报错。出现这种情况的原因、解决方法与 Request 库的进一步了解我们之后再聊。

接下来是下载图片的爬虫。在网上冲浪时，经常能碰见能够生成随机壁纸的网站以及 API。我们来写一个爬虫把图片下载下来。

```python
import requests
response = requests.get('https://www.dmoe.cc/random.php') # 调用某随机图片 API 下载图片
with open("plmm.jpg", "wb+") as f: # 使用 wb+ 来读写图片
    f.write(response.content) # 写入到 plmm.jpg 中
```

发现同文件夹下出现了 `plmm.jpg` 文件。真的有漂亮美眉！

至此，我们对爬虫已经有了初步的了解。下次活动我们会介绍 HTTP 协议和爬虫发送请求的具体过程，虽然非常枯燥，但是非常重要，请一定要认真听。